{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import fitz  # PyMuPDF\n",
    "import base64\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import re\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# --- Configura√ß√£o de UI e Tema ---\n",
    "st.set_page_config(page_title=\"IA do Ndados\", page_icon=\"üìä\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .stApp { background-color: #4B0082; color: white; }\n",
    "    h1, h2, h3, p, div { color: white !important; }\n",
    "    .stChatMessage { background-color: #5D3FD3; border-radius: 10px; }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- Inicializa√ß√£o de Estado ---\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [\n",
    "        {\"role\": \"assistant\", \"content\": \"Ol√°, sou a Data, a IA do Ndados. Fa√ßa o upload das documenta√ß√µes ou propostas em PDF ou tire eventuais d√∫vidas sobre os documentos j√° processados. Posso ajudar a extrair informa√ß√µes, analisar propostas e at√© calcular o ITIP para voc√™!\"}\n",
    "    ]\n",
    "if \"memory\" not in st.session_state:\n",
    "    st.session_state[\"memory\"] = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "if \"vectorstore\" not in st.session_state:\n",
    "    st.session_state[\"vectorstore\"] = None\n",
    "\n",
    "# Configura√ß√£o de Modelos Locais\n",
    "LLM_TEXT = \"llama3\"\n",
    "LLM_VISION = \"llava\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "def extract_pdf_content(uploaded_files):\n",
    "    raw_text_data = []\n",
    "    llava_llm = Ollama(model=LLM_VISION, temperature=0.1)\n",
    "    \n",
    "    for file in uploaded_files:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
    "            tmp.write(file.getvalue())\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        doc = fitz.open(tmp_path)\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text(\"text\")\n",
    "            raw_text_data.append(f\"Documento: {file.name} | P√°gina {page_num}:\\n{text}\")\n",
    "            \n",
    "            images = page.get_images(full=True)\n",
    "            for img_index, img in enumerate(images):\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                img_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "                \n",
    "                try:\n",
    "                    vision_prompt = \"Descreva detalhadamente este fluxograma, arquitetura de dados ou extraia o c√≥digo presente na imagem. Seja t√©cnico.\"\n",
    "                    vision_description = llava_llm.invoke(f\"{vision_prompt} [Imagem anexa processada pelo backend]\") \n",
    "                    raw_text_data.append(f\"Descri√ß√£o de Imagem encontrada na p√°gina {page_num}: {vision_description}\")\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Erro ao processar imagem no LLaVA: {e}\")\n",
    "\n",
    "        os.remove(tmp_path)\n",
    "    return raw_text_data\n",
    "\n",
    "def build_vector_db(text_chunks):\n",
    "    from langchain_core.documents import Document\n",
    "    docs = [Document(page_content=t) for t in text_chunks]\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "    return Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "def get_qa_chain(vectorstore):\n",
    "    llm = Ollama(model=LLM_TEXT, temperature=0.0) # Temperatura ZERO para extra√ß√£o de dados financeiros\n",
    "    return ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 7}), # Maior contexto para capturar propostas longas\n",
    "        memory=st.session_state[\"memory\"]\n",
    "    )\n",
    "\n",
    "# --- Interface ---\n",
    "col1, col2 = st.columns([1, 8])\n",
    "with col1:\n",
    "    if os.path.exists(\"foto.png\"):\n",
    "        st.image(\"foto.png\", width=80)\n",
    "with col2:\n",
    "    st.title(\"IA do Ndados - An√°lise de Documenta√ß√£o\")\n",
    "\n",
    "st.sidebar.header(\"Painel de Controle\")\n",
    "uploaded_files = st.sidebar.file_uploader(\"Upload de PDFs\", type=\"pdf\", accept_multiple_files=True)\n",
    "\n",
    "if st.sidebar.button(\"Processar Base\"):\n",
    "    if uploaded_files:\n",
    "        with st.spinner(\"Extraindo e vetorizando...\"):\n",
    "            text_data = extract_pdf_content(uploaded_files)\n",
    "            vs = build_vector_db(text_data)\n",
    "            st.session_state[\"vectorstore\"] = vs\n",
    "            st.session_state[\"chain\"] = get_qa_chain(vs)\n",
    "            st.success(\"An√°lise documental conclu√≠da.\")\n",
    "    else:\n",
    "        st.sidebar.error(\"Insira arquivos.\")\n",
    "\n",
    "# --- Renderiza√ß√£o do Chat ---\n",
    "for msg in st.session_state[\"messages\"]:\n",
    "    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
    "\n",
    "user_query = st.chat_input(\"Ex: Extraia os valores das propostas e calcule o ITIP.\")\n",
    "\n",
    "if user_query:\n",
    "    st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": user_query})\n",
    "    st.chat_message(\"user\").write(user_query)\n",
    "    \n",
    "    if \"chain\" in st.session_state:\n",
    "        with st.spinner(\"Processando...\"):\n",
    "            \n",
    "            # ROTEAMENTO: Identifica se o usu√°rio quer c√°lculos comerciais\n",
    "            if \"itip\" in user_query.lower() or \"pre√ßo\" in user_query.lower() or \"valor\" in user_query.lower():\n",
    "                prompt_extracao = f\"\"\"\n",
    "                Analise os documentos e extraia TODAS as op√ß√µes comerciais apresentadas.\n",
    "                Voc√™ deve retornar ESTRITAMENTE um ARRAY JSON v√°lido com a estrutura abaixo. N√£o inclua texto fora do JSON.\n",
    "                Se houver valor com desconto, preencha 'preco_desconto', caso contr√°rio deixe null.\n",
    "                [\n",
    "                    {{\n",
    "                        \"nome_escopo\": \"Nome da proposta ou pacote\",\n",
    "                        \"preco_original\": <float>,\n",
    "                        \"preco_desconto\": <float ou null>,\n",
    "                        \"semanas\": <int>,\n",
    "                        \"consultores_dados\": <int>\n",
    "                    }}\n",
    "                ]\n",
    "                Pergunta original do usu√°rio: {user_query}\n",
    "                \"\"\"\n",
    "                \n",
    "                response = st.session_state[\"chain\"].invoke({\"question\": prompt_extracao})\n",
    "                resposta_bruta = response[\"answer\"]\n",
    "                \n",
    "                try:\n",
    "                    # Captura o Array JSON da resposta do LLM usando Regex\n",
    "                    json_str = re.search(r'\\[.*\\]', resposta_bruta, re.DOTALL).group()\n",
    "                    propostas = json.loads(json_str)\n",
    "                    \n",
    "                    answer = \"**An√°lise Comercial e C√°lculo de ITIP:**\\n\\n\"\n",
    "                    \n",
    "                    for prop in propostas:\n",
    "                        nome = prop.get(\"nome_escopo\", \"Escopo N√£o Identificado\")\n",
    "                        preco_orig = prop.get(\"preco_original\", 0)\n",
    "                        preco_desc = prop.get(\"preco_desconto\")\n",
    "                        semanas = prop.get(\"semanas\", 1)\n",
    "                        consultores = prop.get(\"consultores_dados\", 1)\n",
    "                        \n",
    "                        # Define qual pre√ßo usar para o ITIP\n",
    "                        preco_final = preco_desc if preco_desc else preco_orig\n",
    "                        status_desconto = \"Com Desconto\" if preco_desc else \"Sem Desconto\"\n",
    "                        \n",
    "                        # C√°lculo matem√°tico isolado e determin√≠stico\n",
    "                        if semanas > 0 and consultores > 0:\n",
    "                            itip = preco_final / (semanas * consultores)\n",
    "                        else:\n",
    "                            itip = 0\n",
    "                            \n",
    "                        answer += f\"### {nome}\\n\"\n",
    "                        answer += f\"- **Pre√ßo Original:** R$ {preco_orig:,.2f}\\n\"\n",
    "                        if preco_desc:\n",
    "                            answer += f\"- **Pre√ßo com Desconto:** R$ {preco_desc:,.2f}\\n\"\n",
    "                        answer += f\"- **Prazo:** {semanas} semanas | **Consultores:** {consultores}\\n\"\n",
    "                        answer += f\"- **ITIP ({status_desconto}):** R$ {itip:,.2f} por semana/consultor\\n\\n\"\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    answer = f\"O modelo falhou em estruturar os dados para c√°lculo matem√°tico. A complexidade do documento pode ter excedido a capacidade de extra√ß√£o estruturada do modelo local. Resposta bruta da IA: {resposta_bruta}\"\n",
    "\n",
    "            else:\n",
    "                # Fluxo de RAG normal para d√∫vidas qualitativas (equipe, ferramentas)\n",
    "                prompt_qualitativo = f\"Aja como um conselheiro s√™nior. Responda de forma direta e anal√≠tica. {user_query}\"\n",
    "                response = st.session_state[\"chain\"].invoke({\"question\": prompt_qualitativo})\n",
    "                answer = response[\"answer\"]\n",
    "            \n",
    "            st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": answer})\n",
    "            st.chat_message(\"assistant\").write(answer)\n",
    "    else:\n",
    "        st.error(\"Erro: Processe a base vetorial primeiro.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
