{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6abc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import fitz  # PyMuPDF\n",
    "import base64\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import re\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# --- Configura√ß√£o de UI ---\n",
    "st.set_page_config(page_title=\"Data IA do Ndados\", page_icon=\"üìä\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .stApp { background-color: #4B0082; color: white; }\n",
    "    h1, h2, h3, p, div { color: white !important; }\n",
    "    .stChatMessage { background-color: #5D3FD3; border-radius: 10px; }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- Inicializa√ß√£o de Estado ---\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [\n",
    "        {\"role\": \"assistant\", \"content\": \"Ol√°. Sou a Data IA do Ndados. Tire d√∫vidas sobre ferramentas, documenta√ß√µes de projetos e propostas antigas.\"}\n",
    "    ]\n",
    "if \"memory\" not in st.session_state:\n",
    "    st.session_state[\"memory\"] = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "if \"vectorstore\" not in st.session_state:\n",
    "    st.session_state[\"vectorstore\"] = None\n",
    "\n",
    "# Configura√ß√£o de Modelos Locais (Ollama)\n",
    "LLM_TEXT = \"llama3\"\n",
    "LLM_VISION = \"llava\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "def extract_pdf_content(uploaded_files):\n",
    "    raw_text_data = []\n",
    "    llava_llm = Ollama(model=LLM_VISION, temperature=0.1)\n",
    "    \n",
    "    for file in uploaded_files:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
    "            tmp.write(file.getvalue())\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        doc = fitz.open(tmp_path)\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            raw_text_data.append(f\"Doc: {file.name} | P√°g {page_num}:\\n{page.get_text('text')}\")\n",
    "            \n",
    "            # Gargalo de Hardware mantido para extra√ß√£o de fluxogramas\n",
    "            images = page.get_images(full=True)\n",
    "            for img_index, img in enumerate(images):\n",
    "                try:\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    vision_description = llava_llm.invoke(f\"Analise estritamente a arquitetura de dados ou fluxograma da imagem. Descreva tecnicamente. [Processamento Backend Ativo]\") \n",
    "                    raw_text_data.append(f\"Vis√£o Computacional (P√°g {page_num}): {vision_description}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        os.remove(tmp_path)\n",
    "    return raw_text_data\n",
    "\n",
    "def build_vector_db(text_chunks):\n",
    "    from langchain_core.documents import Document\n",
    "    docs = [Document(page_content=t) for t in text_chunks]\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "    return Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "def get_rag_chain(vectorstore):\n",
    "    llm = Ollama(model=LLM_TEXT, temperature=0.1)\n",
    "    return ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}), memory=st.session_state[\"memory\"]\n",
    "    )\n",
    "\n",
    "def route_intent(query, llm):\n",
    "    \"\"\"Roteador Sem√¢ntico\"\"\"\n",
    "    prompt = f\"\"\"Classifique a inten√ß√£o do usu√°rio em ESTRITAMENTE uma das tr√™s palavras:\n",
    "    CALCULO - Se o usu√°rio quer extrair pre√ßos, calcular ITIP ou analisar propostas comerciais.\n",
    "    RAG - Se o usu√°rio pergunta sobre escopo, ferramentas ou equipe de um projeto em PDF.\n",
    "    GERAL - Se o usu√°rio pede dicas de dados, IA, compara cen√°rios hipot√©ticos de forma ampla ou tira d√∫vidas conceituais.\n",
    "    Usu√°rio: {query}\n",
    "    Classifica√ß√£o:\"\"\"\n",
    "    res = llm.invoke(prompt).strip().upper()\n",
    "    if \"CALCULO\" in res: return \"CALCULO\"\n",
    "    elif \"RAG\" in res: return \"RAG\"\n",
    "    else: return \"GERAL\"\n",
    "\n",
    "# --- Interface ---\n",
    "col1, col2 = st.columns([1, 8])\n",
    "with col1:\n",
    "    if os.path.exists(\"foto.png\"):\n",
    "        st.image(\"foto.png\", width=80)\n",
    "with col2:\n",
    "    st.title(\"Data IA do Ndados\")\n",
    "\n",
    "st.sidebar.header(\"Painel de Controle\")\n",
    "uploaded_files = st.sidebar.file_uploader(\"Upload de PDFs (Propostas/Docs)\", type=\"pdf\", accept_multiple_files=True)\n",
    "\n",
    "if st.sidebar.button(\"Indexar Documentos\"):\n",
    "    if uploaded_files:\n",
    "        with st.spinner(\"Extraindo texto, inferindo vis√£o e vetorizando...\"):\n",
    "            text_data = extract_pdf_content(uploaded_files)\n",
    "            vs = build_vector_db(text_data)\n",
    "            st.session_state[\"vectorstore\"] = vs\n",
    "            st.session_state[\"chain\"] = get_rag_chain(vs)\n",
    "            st.success(\"Base operacional. Mem√≥ria carregada.\")\n",
    "    else:\n",
    "        st.sidebar.error(\"Arquivo ausente.\")\n",
    "\n",
    "# --- Execu√ß√£o do Chat ---\n",
    "for msg in st.session_state[\"messages\"]:\n",
    "    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
    "\n",
    "user_query = st.chat_input(\"Insira sua diretriz t√©cnica, d√∫vida ou solicita√ß√£o de c√°lculo...\")\n",
    "\n",
    "if user_query:\n",
    "    st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": user_query})\n",
    "    st.chat_message(\"user\").write(user_query)\n",
    "    \n",
    "    with st.spinner(\"Analisando rota de execu√ß√£o...\"):\n",
    "        llm_router = Ollama(model=LLM_TEXT, temperature=0.0)\n",
    "        \n",
    "        intent = route_intent(user_query, llm_router) if st.session_state[\"vectorstore\"] else \"GERAL\"\n",
    "        answer = \"\"\n",
    "\n",
    "        # ROTA 1: Matem√°tica e Extra√ß√£o Comercial\n",
    "        if intent == \"CALCULO\":\n",
    "            prompt_extracao = f\"\"\"Analise os documentos. Extraia as op√ß√µes comerciais. Retorne ESTRITAMENTE um ARRAY JSON:\n",
    "            [{{ \"nome\": \"Nome\", \"preco_original\": <float>, \"preco_desconto\": <float/null>, \"semanas\": <int>, \"consultores\": <int> }}]\n",
    "            Query: {user_query}\"\"\"\n",
    "            \n",
    "            resposta_bruta = st.session_state[\"chain\"].invoke({\"question\": prompt_extracao})[\"answer\"]\n",
    "            try:\n",
    "                json_str = re.search(r'\\[.*\\]', resposta_bruta, re.DOTALL).group()\n",
    "                propostas = json.loads(json_str)\n",
    "                answer = \"**Relat√≥rio Comercial e ITIP:**\\n\\n\"\n",
    "                \n",
    "                for prop in propostas:\n",
    "                    nome = prop.get(\"nome\", \"Indefinido\")\n",
    "                    preco_final = prop.get(\"preco_desconto\") or prop.get(\"preco_original\", 0)\n",
    "                    semanas = prop.get(\"semanas\", 1)\n",
    "                    consultores = prop.get(\"consultores\", 1)\n",
    "                    \n",
    "                    itip = preco_final / (semanas * consultores) if semanas > 0 and consultores > 0 else 0\n",
    "                        \n",
    "                    answer += f\"### {nome}\\n- **Valor Base Execu√ß√£o:** R$ {preco_final:,.2f}\\n- **Prazo:** {semanas} semanas | **Aloca√ß√£o:** {consultores} consultores\\n- **ITIP Determinado:** R$ {itip:,.2f} / sem-consultor\\n\\n\"\n",
    "            except Exception:\n",
    "                answer = \"Falha estrutural. O modelo n√£o conseguiu isolar o JSON das propostas devido √† complexidade do layout e limita√ß√µes de formata√ß√£o do PDF original.\"\n",
    "\n",
    "        # ROTA 2: Busca Documental (RAG)\n",
    "        elif intent == \"RAG\":\n",
    "            response = st.session_state[\"chain\"].invoke({\"question\": user_query})\n",
    "            answer = response[\"answer\"]\n",
    "\n",
    "        # ROTA 3: Consultoria Livre\n",
    "        else:\n",
    "            prompt_geral = f\"\"\"Atue como um arquiteto de dados e IA s√™nior. Responda de forma direta, t√©cnica e anal√≠tica. \n",
    "            Pergunta: {user_query}\"\"\"\n",
    "            answer = llm_router.invoke(prompt_geral)\n",
    "        \n",
    "        st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": answer})\n",
    "        st.chat_message(\"assistant\").write(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
